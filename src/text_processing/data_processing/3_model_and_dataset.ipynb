{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8c2a48",
   "metadata": {},
   "source": [
    "# Setting up env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a12a07",
   "metadata": {},
   "source": [
    "## Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afea123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb052e6",
   "metadata": {},
   "source": [
    "## Setting up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413cc82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import DATASET_DIR, DATASET_OUTPUT_FILE, SP500_OUTPUT_FILE, OUT_FILTER_FILE, PRE_DATASET_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7440a2a",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05d49438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9516, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger ton dataset parquet\n",
    "dataset_raw = pd.read_parquet(PRE_DATASET_FILE)\n",
    "print(dataset_raw.shape)  # (9516, 3)\n",
    "\n",
    "# Conversion en listes\n",
    "rf_texts = dataset_raw[\"rf\"].astype(str).tolist()\n",
    "mgmt_texts = dataset_raw[\"mgmt\"].astype(str).tolist()\n",
    "labels = torch.tensor(dataset_raw[\"return\"].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feadf75",
   "metadata": {},
   "source": [
    "# Tokenize and Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75a27383",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-pretrain\")\n",
    "\n",
    "def tokenize_and_chunk(texts, tokenizer, max_length=512, n_chunks=10, desc=\"Chunking\"):\n",
    "    \"\"\"\n",
    "    Tokenize une liste de textes en chunks de taille fixe.\n",
    "    \n",
    "    Retourne:\n",
    "        input_ids: [N, n_chunks, max_length]\n",
    "        attention_mask: [N, n_chunks, max_length]\n",
    "    \"\"\"\n",
    "    all_input_ids, all_attention = [], []\n",
    "\n",
    "    for text in tqdm(texts, desc=desc):\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "        # Split en morceaux de max_length\n",
    "        chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "\n",
    "        ids, masks = [], []\n",
    "        for chunk in chunks[:n_chunks]:\n",
    "            attn = [1] * len(chunk)\n",
    "            if len(chunk) < max_length:\n",
    "                pad_len = max_length - len(chunk)\n",
    "                chunk = chunk + [tokenizer.pad_token_id] * pad_len\n",
    "                attn = attn + [0] * pad_len\n",
    "            ids.append(chunk)\n",
    "            masks.append(attn)\n",
    "\n",
    "        # Padding si moins de n_chunks\n",
    "        while len(ids) < n_chunks:\n",
    "            ids.append([tokenizer.pad_token_id] * max_length)\n",
    "            masks.append([0] * max_length)\n",
    "\n",
    "        all_input_ids.append(ids)\n",
    "        all_attention.append(masks)\n",
    "\n",
    "    return torch.tensor(all_input_ids), torch.tensor(all_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520a2a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing RF: 100%|██████████| 9516/9516 [00:42<00:00, 226.27it/s]\n",
      "Tokenizing MGMT: 100%|██████████| 9516/9516 [02:20<00:00, 67.50it/s] \n"
     ]
    }
   ],
   "source": [
    "rf_ids, rf_masks = tokenize_and_chunk(\n",
    "    dataset_raw[\"rf\"].astype(str).tolist(),\n",
    "    tokenizer,\n",
    "    max_length=256,\n",
    "    n_chunks=10,\n",
    "    desc=\"Tokenizing RF\"\n",
    ")\n",
    "\n",
    "mgmt_ids, mgmt_masks = tokenize_and_chunk(\n",
    "    dataset_raw[\"mgmt\"].astype(str).tolist(),\n",
    "    tokenizer,\n",
    "    max_length=256,\n",
    "    n_chunks=10,\n",
    "    desc=\"Tokenizing MGMT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0c8039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9516, 10, 256])\n",
      "torch.Size([9516, 10, 256])\n",
      "torch.Size([9516, 10, 256])\n",
      "torch.Size([9516, 10, 256])\n",
      "torch.Size([9516])\n",
      "tensor([    3,   366,  4338,    48,   177,   298,  2443,   366,  4338,    48,\n",
      "          177,   298,   181,  1945, 25671,     8,    38,    54,   585,    40])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1242, 1914,    8,  111,   28,  117,  104,  140,  450,  585,   16,   60,\n",
      "          73,    9,  104, 1804,   10,  287, 1228,  174])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(rf_ids.shape)      # torch.Size([9516, 100, 512])\n",
    "print(rf_masks.shape)    # torch.Size([9516, 100, 512])\n",
    "print(mgmt_ids.shape)    # torch.Size([9516, 100, 512])\n",
    "print(mgmt_masks.shape)  # torch.Size([9516, 100, 512]\n",
    "print(labels.shape)\n",
    "\n",
    "i = 0  # premier document\n",
    "print(rf_ids[i, 0, :20])   # premiers tokens du premier chunk\n",
    "print(rf_masks[i, 0, :20]) # mask -> devrait être 1 pour les tokens réels, 0 pour les PAD\n",
    "\n",
    "print(rf_ids[i, -1, :20])   # premiers tokens du dernier chunk (100e)\n",
    "print(rf_masks[i, -1, :20]) # mask -> devrait être que des 0 si chunk vide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfea01e",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbada12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"rf_input_ids\": rf_ids,\n",
    "    \"rf_attention_mask\": rf_masks,\n",
    "    \"mgmt_input_ids\": mgmt_ids,\n",
    "    \"mgmt_attention_mask\": mgmt_masks,\n",
    "    \"labels\": labels\n",
    "}, \"finbert_chunks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d2909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Arthurmaffre34/predataset\", data_files=\"pre_dataset.parquet\")\n",
    "# print(dataset.shape)\n",
    "# # Charger en DataFrame Pandas\n",
    "# dataset_raw = dataset[\"train\"].to_pandas()\n",
    "# print(dataset_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e43c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FinBERTChunksDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        # Charger directement le dict de tensors\n",
    "        self.data = torch.load(data_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"labels\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.data[\"mgmt_input_ids\"][idx],        # [n_chunks, 512]\n",
    "            self.data[\"mgmt_attention_mask\"][idx],   # [n_chunks, 512]\n",
    "            self.data[\"rf_input_ids\"][idx],          # [n_chunks, 512]\n",
    "            self.data[\"rf_attention_mask\"][idx],     # [n_chunks, 512]\n",
    "            self.data[\"labels\"][idx]                 # scalaire\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95591933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset\n",
    "dataset = FinBERTChunksDataset(\"finbert_chunks.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1233f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mgmt_ids: torch.Size([1, 10, 256])\n",
      "mgmt_masks: torch.Size([1, 10, 256])\n",
      "rf_ids: torch.Size([1, 10, 256])\n",
      "rf_masks: torch.Size([1, 10, 256])\n",
      "labels: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Créer un DataLoader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Test : vérifier les shapes\n",
    "for mgmt_ids, mgmt_masks, rf_ids, rf_masks, labels in loader:\n",
    "    print(\"mgmt_ids:\", mgmt_ids.shape)       # [B, 100, 512]\n",
    "    print(\"mgmt_masks:\", mgmt_masks.shape)   # [B, 100, 512]\n",
    "    print(\"rf_ids:\", rf_ids.shape)           # [B, 100, 512]\n",
    "    print(\"rf_masks:\", rf_masks.shape)       # [B, 100, 512]\n",
    "    print(\"labels:\", labels.shape)           # [B]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b7418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class MultiStageFinBERT(nn.Module):\n",
    "    def __init__(self, bert_model=\"yiyanghkust/finbert-pretrain\",\n",
    "                 hidden_dim=768, n_heads=4, n_layers=1):\n",
    "        super().__init__()\n",
    "        # BERT partagé (reste en FP32, AMP gérera le cast)\n",
    "        self.bert = AutoModel.from_pretrained(bert_model)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Agrégateur Transformer (reste en FP32)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=n_heads, batch_first=True\n",
    "        )\n",
    "        self.chunk_transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # MLP final\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mgmt_ids, mgmt_masks, rf_ids, rf_masks):\n",
    "        B, N, L = mgmt_ids.shape  # [batch, n_chunks, seq_len]\n",
    "\n",
    "        def run_bert(ids, masks):\n",
    "            ids = ids.view(B*N, L).to(dtype=torch.long)   # IDs restent int64\n",
    "            masks = masks.view(B*N, L).to(dtype=torch.long)\n",
    "\n",
    "            outputs = self.bert(input_ids=ids, attention_mask=masks)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [B*N, hidden_dim]\n",
    "\n",
    "            return cls_embeddings.view(B, N, self.hidden_dim)\n",
    "\n",
    "        mgmt_emb = run_bert(mgmt_ids, mgmt_masks)\n",
    "        rf_emb = run_bert(rf_ids, rf_masks)\n",
    "\n",
    "        # Agrégation des chunks avec Transformer\n",
    "        mgmt_repr = self.chunk_transformer(mgmt_emb).mean(dim=1)  # [B, hidden_dim]\n",
    "        rf_repr = self.chunk_transformer(rf_emb).mean(dim=1)      # [B, hidden_dim]\n",
    "\n",
    "        # Concat + MLP\n",
    "        combined = torch.cat([mgmt_repr, rf_repr], dim=1)  # [B, 2*hidden_dim]\n",
    "        out = self.fc(combined)                            # [B, 1]\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00938cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3869], device='mps:0')\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "model = MultiStageFinBERT().to(device)\n",
    "\n",
    "for mgmt_ids, mgmt_masks, rf_ids, rf_masks, labels in loader:\n",
    "    mgmt_ids, mgmt_masks = mgmt_ids.to(device), mgmt_masks.to(device)\n",
    "    rf_ids, rf_masks = rf_ids.to(device), rf_masks.to(device)\n",
    "    labels = labels.to(device)\n",
    "    print(labels)\n",
    "    preds = model(mgmt_ids, mgmt_masks, rf_ids, rf_masks)\n",
    "    print(preds.shape)  # [B]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280f0d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 116,446,977\n",
      "Paramètres entraînables : 116,446,977\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Nombre total de paramètres : {total_params:,}\")\n",
    "print(f\"Paramètres entraînables : {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f3e9000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/px25gn9n1cg8clmw10nxnsg00000gn/T/ipykernel_27259/647210869.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Users/arthur/miniconda3/envs/ECN6338/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/var/folders/3d/px25gn9n1cg8clmw10nxnsg00000gn/T/ipykernel_27259/647210869.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/Users/arthur/miniconda3/envs/ECN6338/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     loss = criterion(preds, labels)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# backward AMP\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m scaler.step(optimizer)\n\u001b[32m     42\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ECN6338/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ECN6338/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ECN6338/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dataset / DataLoader\n",
    "dataset = FinBERTChunksDataset(\"finbert_chunks.pt\")\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "# Modèle + optim\n",
    "model = MultiStageFinBERT().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# AMP\n",
    "scaler = GradScaler()\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, (mgmt_ids, mgmt_masks, rf_ids, rf_masks, labels) in enumerate(loader):\n",
    "        mgmt_ids, mgmt_masks = mgmt_ids.to(device), mgmt_masks.to(device)\n",
    "        rf_ids, rf_masks = rf_ids.to(device), rf_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # AMP forward\n",
    "        with autocast():\n",
    "            preds = model(mgmt_ids, mgmt_masks, rf_ids, rf_masks)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "        # backward AMP\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1} Step {step}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1} finished - Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device=\"mps\", n_show=5):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds_list, labels_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            rf_ids = batch[\"rf_input_ids\"].to(device)\n",
    "            rf_mask = batch[\"rf_attention_mask\"].to(device)\n",
    "            mgmt_ids = batch[\"mgmt_input_ids\"].to(device)\n",
    "            mgmt_mask = batch[\"mgmt_attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(rf_ids, rf_mask, mgmt_ids, mgmt_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds_list.extend(outputs.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Corrélation Pearson\n",
    "    preds_arr = np.array(preds_list)\n",
    "    labels_arr = np.array(labels_list)\n",
    "    if len(np.unique(labels_arr)) > 1:  # éviter division par zéro\n",
    "        corr = np.corrcoef(preds_arr, labels_arr)[0, 1]\n",
    "    else:\n",
    "        corr = float(\"nan\")\n",
    "\n",
    "    print(f\"✅ Evaluation - Loss moyenne: {avg_loss:.4f}, Corrélation: {corr:.4f}\")\n",
    "\n",
    "    # Exemples\n",
    "    print(\"\\nExemples de prédictions vs labels:\")\n",
    "    for i in range(min(n_show, len(preds_list))):\n",
    "        print(f\"  Pred: {preds_list[i]:.4f} | Label: {labels_list[i]:.4f}\")\n",
    "\n",
    "    return avg_loss, corr, preds_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e8f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:39<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation - Loss moyenne: 0.0186, Corrélation: -0.0369\n",
      "\n",
      "Exemples de prédictions vs labels:\n",
      "  Pred: 0.0494 | Label: 0.0628\n",
      "  Pred: 0.0594 | Label: 0.1351\n",
      "  Pred: 0.0405 | Label: 0.0051\n",
      "  Pred: 0.0284 | Label: -0.1686\n",
      "  Pred: 0.0545 | Label: 0.2264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Supposons que tu aies déjà : model, criterion, loader_small, device\n",
    "loss, corr, preds, labels = evaluate_model(model, eval_loader, criterion, device=\"mps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECN6338",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
