# üìö Pipeline de pr√©paration du dataset FinBERT

Ce projet contient deux √©tapes principales pour transformer les rapports financiers bruts en tenseurs exploitables par un mod√®le FinBERT.

## ‚öôÔ∏è √âtape 1 ‚Äì Pr√©paration des donn√©es brutes

Script : prep_text_dataset.py
- Charge les fichiers text_us_YYYY.pkl (2005‚Äì2025).
- Concat√®ne toutes les ann√©es en un seul DataFrame Polars.
- Filtre les rapports :
- uniquement les tickers S&P500
- uniquement les 10Q
- longueur ‚â§ 250 000 caract√®res
- rf et mgmt non vides
- garde uniquement les CIK les plus fr√©quents (mode)
- Ajoute les m√©tadonn√©es du S&P500 (ticker, security).
- V√©rifie la coh√©rence du dataset.
- Sauvegarde en Parquet (text_dataset.parquet).

üëâ Ex√©cution :

```bash
python src/text_processing/prep_text_dataset.py
```
## ‚öôÔ∏è √âtape 2 ‚Äì Tokenization & Chunking

Script : prep_dataset.py
- Charge le dataset filtr√© depuis Hugging Face ou un fichier parquet local.
- Tokenize les textes avec FinBERT.
- D√©coupe chaque texte en chunks de taille fixe (par d√©faut 256 tokens √ó 10 chunks).
- Produit des tenseurs PyTorch (input_ids, attention_masks, labels).
- Sauvegarde en .pt (finbert_chunks.pt).

üëâ Ex√©cution :

```bash
python src/text_processing/prep_dataset.py \
    --repo_id "Arthurmaffre34/predataset" \
    --data_file "pre_dataset.parquet" \
    --max_length 256 \
    --n_chunks 10 \
    --save_path "finbert_chunks.pt"
```

# üìÇ Gestion des chemins (datasets)

Nous utilisons **`pathlib`** pour d√©finir des chemins **dynamiques et portables**, afin que le code fonctionne sur n‚Äôimporte quelle machine sans modifier les chemins manuellement.

```python
from pathlib import Path

# Base du projet (2 niveaux au-dessus du script courant)
BASE_DIR = Path.cwd().parents[2]

# R√©pertoire datasets
DATASET_DIR = BASE_DIR / "src" / "text_processing" / "text_dataset"

# Fichiers utilis√©s dans le pipeline
DATASET_OUTPUT_FILE = DATASET_DIR / "text_dataset.parquet"
SP500_OUTPUT_FILE   = DATASET_DIR / "sp500.parquet"
OUT_FILTER_FILE     = DATASET_DIR / "out_filter_dataset.parquet"
PRE_DATASET_FILE    = DATASET_DIR / "pre_dataset.parquet"

if __name__ == "__main__":
    print("Base dir:", BASE_DIR)
    print("Dataset dir:", DATASET_DIR)
```
## Exemple d'utilisation

```python
import pandas as pd
from pathlib import Path

from config_paths import PRE_DATASET_FILE  # <- ton module avec les chemins

# Charger le dataset pr√©trait√©
df = pd.read_parquet(PRE_DATASET_FILE)
print("Shape:", df.shape)
print(df.head())
```


### ‚úÖ Avantages
- **√âvolutif** : pas de chemins absolus ‚Üí portable partout.  
- **Centralis√©** : tous les chemins importants au m√™me endroit.  
- **Clair** : on sait exactement o√π sont stock√©s les datasets.  
